{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento Óptico de Caracteres (OCR) com Detecção de Contorno\n",
    "\n",
    "Este notebook demonstra um fluxo de trabalho completo de OCR em uma imagem de amostra. O processo começa carregando um modelo de rede neural convolucional (CNN) pré-treinado, projetado para reconhecer caracteres individuais. Em seguida, a imagem de entrada é pré-processada para otimizar a detecção de caracteres. A etapa principal é a segmentação de caracteres, que usa a detecção de contorno do OpenCV para isolar cada caractere na imagem. Finalmente, cada caractere segmentado é preparado e passado para o modelo para previsão. O notebook conclui exibindo o texto reconhecido e visualizando os resultados com caixas delimitadoras e rótulos de caracteres na imagem original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(markers=(\"pyproject.toml\", \".git\")) -> Path:\n",
    "    \"\"\"Localiza a pasta raiz do projeto.\n",
    "\n",
    "    Args:\n",
    "        markers (tuple, optional): Uma tupla de nomes de arquivos ou pastas que marcam a raiz do projeto.\n",
    "                                     O padrão é (\"pyproject.toml\", \".git\").\n",
    "\n",
    "    Returns:\n",
    "        Path: O objeto Path para o diretório raiz do projeto.\n",
    "    \"\"\"\n",
    "    p = Path.cwd().resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if any((parent / m).exists() for m in markers):\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "ROOT = find_project_root()\n",
    "print(f\"Raiz do projeto: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações\n",
    "\n",
    "Define os caminhos para os arquivos de dados, o modelo treinado, a imagem de teste e as classes, além de definir as dimensões das imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODEL_PATH = DATA_DIR / \"processed\" / \"modelo_ocr_simbolos.keras\"\n",
    "IMAGE_PATH = DATA_DIR / \"raw\" / \"ocr_test_image.png\"\n",
    "CLASS_NAMES_PATH = DATA_DIR / \"raw\" / \"class_names.json\"\n",
    "\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo\n",
    "\n",
    "Carrega o modelo de OCR treinado e exibe um resumo da sua arquitetura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo treinado\n",
    "print(\"Carregando modelo...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"✓ Modelo carregado com sucesso!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos Nomes das Classes\n",
    "\n",
    "Carrega os nomes das classes (caracteres) do arquivo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os nomes das classes\n",
    "with open(CLASS_NAMES_PATH, 'r', encoding='utf-8') as f:\n",
    "    class_names = json.load(f)\n",
    "print(f\"✓ {len(class_names)} classes carregadas\")\n",
    "print(f\"Exemplos: {class_names[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_char(unicode_str: str) -> str:\n",
    "    \"\"\"Converte uma string unicode no formato 'U+XXXX' para o caractere correspondente.\n",
    "\n",
    "    Args:\n",
    "        unicode_str (str): A string no formato 'U+XXXX'.\n",
    "\n",
    "    Returns:\n",
    "        str: O caractere correspondente ou '?' em caso de erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        codepoint = int(unicode_str.replace('U+', ''), 16)\n",
    "        return chr(codepoint)\n",
    "    except:\n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento da Imagem de Teste\n",
    "\n",
    "Carrega e exibe a imagem que será utilizada para o OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e exibe a imagem original\n",
    "print(f\"Carregando imagem: {IMAGE_PATH}\")\n",
    "original_image = Image.open(IMAGE_PATH)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Imagem Original para OCR')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dimensões: {original_image.size}\")\n",
    "print(f\"Modo: {original_image.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento da Imagem\n",
    "\n",
    "A imagem é convertida para escala de cinza, binarizada usando um threshold adaptativo e, se necessário, invertida para garantir que o texto seja preto e o fundo branco, que é o formato esperado pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_for_ocr(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\" Pré-processa a imagem para o OCR.\n",
    "\n",
    "    Aplica as seguintes etapas:\n",
    "    - Converte a imagem para escala de cinza.\n",
    "    - Aplica um threshold adaptativo para binarizar a imagem.\n",
    "    - Inverte as cores, se necessário, para garantir um fundo branco e texto preto.\n",
    "\n",
    "    Args:\n",
    "        image (Image.Image): A imagem de entrada.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A imagem pré-processada como um array NumPy.\n",
    "    \"\"\"\n",
    "    # Converte para escala de cinza\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "\n",
    "    # Converte para numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # Aplica threshold adaptativo para melhor binarização\n",
    "    img_binary = cv2.adaptiveThreshold(\n",
    "        img_array, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "\n",
    "    # Verifica se precisa inverter (modelo espera fundo branco, texto preto)\n",
    "    # Se a média é baixa, a imagem está com fundo escuro\n",
    "    if np.mean(img_binary) < 127:\n",
    "        img_binary = 255 - img_binary\n",
    "\n",
    "    return img_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b11006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processa a imagem\n",
    "print(\"Pré-processando imagem...\")\n",
    "processed_image = preprocess_image_for_ocr(original_image)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(processed_image, cmap='gray')\n",
    "plt.title('Imagem Pré-processada (Binarizada)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentação de Caracteres\n",
    "\n",
    "Utiliza a detecção de contornos para segmentar (isolar) cada caractere individual na imagem pré-processada. Os caracteres são então ordenados da esquerda para a direita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters(image_array: np.ndarray) -> list:\n",
    "    \"\"\"Segmenta os caracteres individuais da imagem binarizada.\n",
    "\n",
    "    Esta função utiliza o algoritmo de detecção de contornos do OpenCV para encontrar os limites\n",
    "    de cada caractere. Os contornos são então filtrados para remover ruídos e ordenados \n",
    "    da esquerda para a direita com base em sua posição no eixo x.\n",
    "\n",
    "    Args:\n",
    "        image_array (np.ndarray): Uma matriz NumPy representando a imagem binarizada \n",
    "                                (fundo branco, texto preto).\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tuplas, onde cada tupla contém:\n",
    "              - Uma tupla (x, y, w, h) representando a caixa delimitadora do caractere.\n",
    "              - Uma matriz NumPy da imagem do caractere segmentado com preenchimento.\n",
    "    \"\"\"\n",
    "    # Encontra contornos na imagem. A imagem é invertida (255 - image_array) \n",
    "    # porque a função `findContours` espera que os objetos sejam brancos em um fundo preto.\n",
    "    contours, _ = cv2.findContours(\n",
    "        255 - image_array,  # Inverte para findContours\n",
    "        cv2.RETR_EXTERNAL, # Recupera apenas os contornos externos\n",
    "        cv2.CHAIN_APPROX_SIMPLE # Comprime segmentos horizontais, verticais e diagonais e deixa apenas seus pontos finais\n",
    "    )\n",
    "\n",
    "    # Filtra e ordena os contornos da esquerda para a direita\n",
    "    char_regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Filtra contornos muito pequenos que provavelmente são ruído\n",
    "        if w > 1 and h > 1:\n",
    "            char_regions.append((x, y, w, h))\n",
    "\n",
    "    # Ordena as regiões de caracteres com base na coordenada x (da esquerda para a direita)\n",
    "    char_regions = sorted(char_regions, key=lambda r: r[0])\n",
    "\n",
    "    # Extrai as regiões da imagem para cada caractere\n",
    "    segmented_chars = []\n",
    "    for (x, y, w, h) in char_regions:\n",
    "        # Adiciona um preenchimento (padding) ao redor de cada caractere para garantir \n",
    "        # que ele não seja cortado\n",
    "        padding = 5\n",
    "        x1 = max(0, x - padding)\n",
    "        y1 = max(0, y - padding)\n",
    "        x2 = min(image_array.shape[1], x + w + padding)\n",
    "        y2 = min(image_array.shape[0], y + h + padding)\n",
    "\n",
    "        char_img = image_array[y1:y2, x1:x2]\n",
    "        segmented_chars.append(((x, y, w, h), char_img))\n",
    "\n",
    "    return segmented_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872316f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenta caracteres\n",
    "print(\"Segmentando caracteres...\")\n",
    "segmented_chars = segment_characters(processed_image)\n",
    "print(f\"✓ {len(segmented_chars)} caracteres detectados\")\n",
    "\n",
    "# Visualiza caracteres segmentados\n",
    "if len(segmented_chars) > 0:\n",
    "    n_chars = len(segmented_chars)\n",
    "    cols = min(10, n_chars)\n",
    "    rows = (n_chars + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(15, 2 * rows))\n",
    "    for i, (bbox, char_img) in enumerate(segmented_chars):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(char_img, cmap='gray')\n",
    "        plt.title(f'#{i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Caracteres Segmentados')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠ Nenhum caractere foi detectado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação para Predição\n",
    "\n",
    "Redimensiona e centraliza cada caractere segmentado para o formato esperado pelo modelo (64x64 pixels), preparando-o para a predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_char_for_prediction(char_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Prepara a imagem de um caractere segmentado para a predição do modelo.\n",
    "\n",
    "    Esta função pega a imagem de um único caractere, a centraliza em uma tela quadrada \n",
    "    (canvas) branca, redimensiona para as dimensões esperadas pelo modelo (64x64) e \n",
    "    remodela o array para o formato de entrada do modelo (1, 64, 64, 1).\n",
    "\n",
    "    Args:\n",
    "        char_image (np.ndarray): A imagem do caractere segmentado (fundo branco).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A imagem preparada, pronta para ser usada como entrada para o \n",
    "                    modelo de previsão.\n",
    "    \"\"\"\n",
    "    # Cria um canvas quadrado branco do tamanho do maior lado da imagem\n",
    "    size = max(char_image.shape)\n",
    "    canvas = np.ones((size, size), dtype=np.uint8) * 255\n",
    "\n",
    "    # Calcula os deslocamentos para centralizar o caractere no canvas\n",
    "    y_offset = (size - char_image.shape[0]) // 2\n",
    "    x_offset = (size - char_image.shape[1]) // 2\n",
    "    # Coloca o caractere no centro do canvas\n",
    "    canvas[y_offset:y_offset+char_image.shape[0],\n",
    "           x_offset:x_offset+char_image.shape[1]] = char_image\n",
    "\n",
    "    # Redimensiona o canvas para as dimensões de entrada do modelo (64x64)\n",
    "    # A interpolação LANCZOS4 é usada por ser boa para reduzir o tamanho de imagens\n",
    "    resized = cv2.resize(canvas, (IMG_WIDTH, IMG_HEIGHT),\n",
    "                        interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # Adiciona as dimensões do lote (batch) e do canal (channel) para corresponder ao formato de \n",
    "    # entrada do modelo: (1, 64, 64, 1)\n",
    "    # A normalização (divisão por 255) não é feita aqui porque o modelo possui uma camada de \n",
    "    # \"Rescaling(1./255)\" como primeira camada.\n",
    "    prepared = resized.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza OCR em cada caractere\n",
    "print(\"\\nRealizando OCR...\")\n",
    "results = []\n",
    "\n",
    "for i, (bbox, char_img) in enumerate(segmented_chars):\n",
    "    # Prepara para predição\n",
    "    input_array = prepare_char_for_prediction(char_img)\n",
    "\n",
    "    # Predição\n",
    "    predictions = model.predict(input_array, verbose=0)\n",
    "    predicted_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_idx]\n",
    "\n",
    "    # Converte para caractere\n",
    "    unicode_class = class_names[predicted_idx]\n",
    "    character = unicode_to_char(unicode_class)\n",
    "\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'bbox': bbox,\n",
    "        'unicode': unicode_class,\n",
    "        'character': character,\n",
    "        'confidence': confidence,\n",
    "        'image': char_img\n",
    "    })\n",
    "\n",
    "    print(f\"Caractere {i+1}: '{character}' ({unicode_class}) - Confiança: {confidence:.4f}\")\n",
    "\n",
    "# Texto reconhecido\n",
    "recognized_text = ''.join([r['character'] for r in results])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEXTO RECONHECIDO: {recognized_text}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização detalhada dos resultados\n",
    "if len(results) > 0:\n",
    "    n_results = len(results)\n",
    "    cols = min(8, n_results)\n",
    "    rows = (n_results + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(16, 3 * rows))\n",
    "    for i, result in enumerate(results):\n",
    "        # Imagem preparada para o modelo\n",
    "        prepared = prepare_char_for_prediction(result['image'])\n",
    "\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(prepared.squeeze(), cmap='gray')\n",
    "        plt.title(\n",
    "            f\"'{result['character']}'\\n\"\n",
    "            f\"{result['unicode']}\\n\"\n",
    "            f\"Conf: {result['confidence']:.2%}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(\n",
    "        f'Resultados do OCR: \"{recognized_text}\"',\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagem com anotações\n",
    "annotated = cv2.cvtColor(processed_image.copy(), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "for result in results:\n",
    "    x, y, w, h = result['bbox']\n",
    "    # Desenha retângulo\n",
    "    cv2.rectangle(annotated, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    # Adiciona texto\n",
    "    cv2.putText(\n",
    "        annotated, result['character'],\n",
    "        (x, y-5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (255, 0, 0), 2\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(annotated)\n",
    "plt.title(f'OCR Completo: \"{recognized_text}\"')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva resultados\n",
    "output_data = {\n",
    "    'recognized_text': recognized_text,\n",
    "    'total_characters': len(results),\n",
    "    'average_confidence': float(np.mean([r['confidence'] for r in results])),\n",
    "    'characters': [\n",
    "        {\n",
    "            'position': i,\n",
    "            'character': r['character'],\n",
    "            'unicode': r['unicode'],\n",
    "            'confidence': float(r['confidence']),\n",
    "            'bbox': r['bbox']\n",
    "        }\n",
    "        for i, r in enumerate(results)\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_path = DATA_DIR / \"processed\" / \"ocr_results.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Resultados salvos em: {output_path}\")\n",
    "print(f\"\\nResumo:\")\n",
    "print(f\"  Texto: {recognized_text}\")\n",
    "print(f\"  Caracteres: {len(results)}\")\n",
    "print(f\"  Confiança média: {output_data['average_confidence']:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
