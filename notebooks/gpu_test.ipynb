{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6fbd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 08:52:54.872337: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 08:52:55.379221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 08:52:56.897945: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf942ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.14 | packaged by conda-forge | (main, Oct 13 2025, 14:09:32) [GCC 14.3.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5e53b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "  GPUs detectadas por TensorFlow: 1\n",
      "    [0] PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "  TF built with CUDA: True\n",
      "\n",
      "Versão CUDA: 12.5.1\n",
      "Versão cuDNN: 9\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"  GPUs detectadas por TensorFlow:\", len(gpus))\n",
    "    for i, g in enumerate(gpus):\n",
    "        print(f\"    [{i}] {g}\")\n",
    "    try:\n",
    "        print(\"  TF built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    except Exception:\n",
    "        # fallback: algumas versões não têm is_built_with_cuda\n",
    "        print(\"  TF built with CUDA: (não disponível nesta versão do TF)\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow: não disponível ou erro ao importar:\", e)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "build = tf.sysconfig.get_build_info()\n",
    "print(\"Versão CUDA:\", build.get(\"cuda_version\", \"desconhecida\"))\n",
    "print(\"Versão cuDNN:\", build.get(\"cudnn_version\", \"desconhecida\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0087ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1\n",
      "  torch.cuda.is_available(): True\n",
      "  GPUs detectadas por PyTorch: 1\n",
      "    [0] NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "      Memória total (bytes): 8585216000\n",
      "  CUDA (compilado): 12.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"  torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "    try:\n",
    "        count = torch.cuda.device_count()\n",
    "        print(\"  GPUs detectadas por PyTorch:\", count)\n",
    "        for i in range(count):\n",
    "            print(f\"    [{i}] {torch.cuda.get_device_name(i)}\")\n",
    "            mem = torch.cuda.get_device_properties(i).total_memory\n",
    "            print(f\"      Memória total (bytes): {mem}\")\n",
    "    except Exception as e:\n",
    "        print(\"  Erro ao obter detalhes da GPU via PyTorch:\", e)\n",
    "    print(\"  CUDA (compilado):\", torch.version.cuda)\n",
    "except Exception as e:\n",
    "    print(\"PyTorch: não disponível ou erro ao importar:\", e)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1e64cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi -L (GPUs detectadas):\n",
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU (UUID: GPU-4cee1ad4-4832-56fc-971f-b46a3bae8f51)\n",
      "nvidia-smi (sumário):\n",
      "name, index, memory.total [MiB], driver_version\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU, 0, 8188 MiB, 581.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_cmd(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)\n",
    "        return out.decode('utf-8', errors='ignore')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return None\n",
    "\n",
    "nvsmi = run_cmd(\"nvidia-smi -L\")\n",
    "if nvsmi:\n",
    "    print(\"nvidia-smi -L (GPUs detectadas):\")\n",
    "    print(nvsmi.strip())\n",
    "    print(\"nvidia-smi (sumário):\")\n",
    "    print(run_cmd(\"nvidia-smi --query-gpu=name,index,memory.total,driver_version --format=csv\"))\n",
    "else:\n",
    "    print(\"nvidia-smi não encontrado ou não respondeu. Pode não haver driver NVIDIA instalado ou o utilitário não está no PATH.\")\n",
    "    # tentar informações do kernel\n",
    "    mod = run_cmd(\"lsmod | grep nvidia || true\")\n",
    "    print(\"Módulos nvidia carregados (lsmod | grep nvidia):\")\n",
    "    print(mod or \"(nenhum)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3b285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# cria device explicitamente (assumindo que cuda está disponível)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca089a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exemplo: mover modelo e batch\n",
    "model = MyModel()\n",
    "model.to(device)                   # move pesos para GPU 0\n",
    "\n",
    "# quando receber batch\n",
    "images = images.to(device, non_blocking=True)\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22affb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula: verificação completa de GPU (TensorFlow, PyTorch, nvidia-smi)\n",
    "import sys\n",
    "import subprocess\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "def hr(title=\"\"):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if title:\n",
    "        print(title)\n",
    "        print(\"-\"*len(title))\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, timeout=10)\n",
    "        return out.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def human_bytes(n):\n",
    "    # formata bytes em GB/MB\n",
    "    for unit in ['B','KB','MB','GB','TB']:\n",
    "        if abs(n) < 1024.0:\n",
    "            return f\"{n:3.1f}{unit}\"\n",
    "        n /= 1024.0\n",
    "    return f\"{n:.1f}PB\"\n",
    "\n",
    "print(f\"Relatório gerado em: {datetime.now().isoformat(sep=' ', timespec='seconds')}\")\n",
    "hr(\"Python / Ambiente\")\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "\n",
    "# TensorFlow\n",
    "hr(\"TensorFlow\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"tensorflow:\", tf.__version__)\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        print(\"  GPUs detectadas por TensorFlow:\", len(gpus))\n",
    "        for i, g in enumerate(gpus):\n",
    "            print(f\"    [{i}] {g}\")\n",
    "    except Exception as e:\n",
    "        print(\"  Erro ao listar GPUs no TF:\", e)\n",
    "    try:\n",
    "        built_cuda = tf.test.is_built_with_cuda()\n",
    "        print(\"  TF built with CUDA:\", built_cuda)\n",
    "    except Exception:\n",
    "        # fallback se função não existir\n",
    "        build = tf.sysconfig.get_build_info()\n",
    "        print(\"  TF build info (cuda/cudnn):\",\n",
    "              f\"cuda={build.get('cuda_version','?')}, cudnn={build.get('cudnn_version','?')}\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow não disponível / erro ao importar:\", str(e))\n",
    "\n",
    "# PyTorch\n",
    "hr(\"PyTorch\")\n",
    "try:\n",
    "    import torch\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    cuda_avail = torch.cuda.is_available()\n",
    "    print(\"  torch.cuda.is_available():\", cuda_avail)\n",
    "    try:\n",
    "        dev_count = torch.cuda.device_count()\n",
    "        print(\"  GPUs detectadas por PyTorch:\", dev_count)\n",
    "        for i in range(dev_count):\n",
    "            try:\n",
    "                name = torch.cuda.get_device_name(i)\n",
    "            except Exception:\n",
    "                name = \"(nome indisponível)\"\n",
    "            try:\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                mem = props.total_memory\n",
    "            except Exception:\n",
    "                mem = None\n",
    "            print(f\"    [{i}] {name}\")\n",
    "            if mem:\n",
    "                print(f\"       Memória total: {human_bytes(mem)}\")\n",
    "    except Exception as e:\n",
    "        print(\"  Erro ao obter detalhes via PyTorch:\", e)\n",
    "    try:\n",
    "        print(\"  CUDA (compilado):\", torch.version.cuda)\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print(\"PyTorch não disponível / erro ao importar:\", str(e))\n",
    "\n",
    "# nvidia-smi / driver\n",
    "hr(\"nvidia-smi / Driver\")\n",
    "nvsmi_path = run_cmd(\"which nvidia-smi\")\n",
    "if nvsmi_path:\n",
    "    print(\"nvidia-smi encontrado em:\", nvsmi_path)\n",
    "    print(\"\\nSaída: nvidia-smi -L\")\n",
    "    print(run_cmd(\"nvidia-smi -L\") or \"(vazio)\")\n",
    "    print(\"\\nSumário (name,index,memory.total,driver_version):\")\n",
    "    print(run_cmd(\"nvidia-smi --query-gpu=name,index,memory.total,driver_version --format=csv\") or \"(não retornou)\")\n",
    "    print(\"\\nUtilização / status (top):\")\n",
    "    print(run_cmd(\"nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,utilization.memory --format=csv,nounits\") or \"(não retornou)\")\n",
    "else:\n",
    "    print(\"nvidia-smi não encontrado no PATH. Pode não haver driver NVIDIA instalado ou utilitário não está disponível.\")\n",
    "    # tenta checar módulos do kernel\n",
    "    mod = run_cmd(\"lsmod | grep -i nvidia || true\")\n",
    "    print(\"\\nMódulos nvidia carregados (lsmod | grep -i nvidia):\")\n",
    "    print(mod or \"(nenhum)\")\n",
    "\n",
    "# Dicas rápidas\n",
    "hr(\"Dicas rápidas / interpretação\")\n",
    "print(textwrap.dedent(\"\"\"\n",
    "- Se 'nvidia-smi' não existir: driver NVIDIA ausente; instale driver apropriado para sua distribuição.\n",
    "- Se 'nvidia-smi' mostrar GPUs, mas torch.cuda.is_available() for False:\n",
    "  - possivelmente PyTorch foi instalado sem suporte CUDA ou há incompatibilidade entre driver e runtime CUDA.\n",
    "  - solução rápida: usar conda e instalar pytorch + pytorch-cuda compatível com sua versão de driver.\n",
    "- Se TensorFlow não vê GPU: verifique se instalou a versão GPU (ou compatível) do TensorFlow e a compatibilidade CUDA/cuDNN.\n",
    "- Em Docker: rode com '--gpus all' e instale 'nvidia-docker2' / 'nvidia-container-toolkit'.\n",
    "- Se quiser, cole a saída completa desta célula aqui que eu indico os comandos exatos para instalar drivers / bibliotecas.\n",
    "\"\"\"))\n",
    "hr(\"Fim do relatório\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
