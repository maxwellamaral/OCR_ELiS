{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento Óptico de Caracteres (OCR) com Agrupamento de Contornos\n",
    "\n",
    "Este notebook demonstra um fluxo de trabalho de OCR aprimorado, focado em lidar com caracteres que são compostos por vários contornos separados (por exemplo, 'i', 'j', '=', ou caracteres acentuados). A abordagem principal é agrupar contornos que estão próximos horizontalmente antes de realizar o reconhecimento.\n",
    "\n",
    "O processo inclui:\n",
    "1.  **Pré-processamento de Imagem**: Binarização da imagem de entrada para isolar o texto.\n",
    "2.  **Análise de Espaçamento**: Análise estatística dos espaços entre os contornos para sugerir um limiar ideal (`max_gap`) para o agrupamento.\n",
    "3.  **Segmentação com Agrupamento**: Identificação e agrupamento de contornos próximos para formar símbolos completos.\n",
    "4.  **Previsão**: Uso de um modelo CNN pré-treinado para classificar cada símbolo agrupado.\n",
    "5.  **Visualização**: Exibição dos resultados, destacando os símbolos compostos por múltiplos componentes.\n",
    "\n",
    "Esta técnica melhora a precisão do OCR para fontes e idiomas onde os caracteres não são sempre um único componente conectado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localização da Raiz do Projeto\n",
    "\n",
    "A função `find_project_root` sobe na árvore de diretórios a partir do local atual para encontrar um arquivo marcador (como `pyproject.toml` ou `.git`), garantindo que os caminhos para os dados e modelos sejam sempre relativos à raiz do projeto, independentemente de onde o notebook é executado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(markers=(\"pyproject.toml\", \".git\")) -> Path:\n",
    "    \"\"\"Localiza o diretório raiz do projeto.\n",
    "\n",
    "    Esta função sobe na árvore de diretórios a partir do diretório de trabalho atual\n",
    "    até encontrar um arquivo ou pasta que sirva como marcador da raiz do projeto (por exemplo, \n",
    "    `.git` ou `pyproject.toml`). Isso garante que os caminhos para os arquivos de dados sejam \n",
    "    resolvidos corretamente, não importa de onde o script seja executado.\n",
    "\n",
    "    Args:\n",
    "        markers (tuple, optional): Uma tupla de nomes de arquivos/pastas que indicam a raiz.\n",
    "                                     O padrão é (\"pyproject.toml\", \".git\").\n",
    "\n",
    "    Returns:\n",
    "        Path: O caminho absoluto para o diretório raiz do projeto.\n",
    "    \"\"\"\n",
    "    p = Path.cwd().resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if any((parent / m).exists() for m in markers):\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "ROOT = find_project_root()\n",
    "print(f\"Raiz do projeto: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração de Caminhos e Parâmetros\n",
    "\n",
    "Define todos os caminhos de arquivo necessários, como o local do modelo, a imagem de teste e o arquivo de nomes de classes. Também define as dimensões de imagem (`IMG_HEIGHT`, `IMG_WIDTH`) que o modelo espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODEL_PATH = DATA_DIR / \"processed\" / \"modelo_ocr_simbolos.keras\"\n",
    "IMAGE_PATH = DATA_DIR / \"raw\" / \"ocr_test_image.png\"\n",
    "CLASS_NAMES_PATH = DATA_DIR / \"raw\" / \"class_names.json\"\n",
    "\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo e Nomes das Classes\n",
    "\n",
    "Carrega o modelo de OCR Keras pré-treinado e o arquivo JSON que mapeia os índices de saída do modelo para os nomes de classes (caracteres Unicode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo treinado\n",
    "print(\"Carregando modelo...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"✓ Modelo carregado com sucesso!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os nomes das classes\n",
    "with open(CLASS_NAMES_PATH, 'r', encoding='utf-8') as f:\n",
    "    class_names = json.load(f)\n",
    "print(f\"✓ {len(class_names)} classes carregadas\")\n",
    "print(f\"Exemplos: {class_names[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Utilitária para Conversão de Unicode\n",
    "\n",
    "A função `unicode_to_char` converte uma representação de string de um ponto de código Unicode (por exemplo, `'U+0041'`) no caractere correspondente (por exemplo, `'A'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_char(unicode_str: str) -> str:\n",
    "    \"\"\"Converte uma string de ponto de código Unicode para um caractere.\n",
    "\n",
    "    Lida com o formato 'U+XXXX' e o converte no caractere de texto correspondente.\n",
    "\n",
    "    Args:\n",
    "        unicode_str (str): A string representando o ponto de código (ex: 'U+0041').\n",
    "\n",
    "    Returns:\n",
    "        str: O caractere correspondente (ex: 'A') ou '?' se ocorrer um erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        codepoint = int(unicode_str.replace('U+', ''), 16)\n",
    "        return chr(codepoint)\n",
    "    except:\n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento e Pré-processamento da Imagem\n",
    "\n",
    "A imagem de teste é carregada e exibida. Em seguida, a função `preprocess_image_for_ocr` a converte para escala de cinza, aplica um limiar adaptativo para binarizá-la e inverte as cores para garantir um fundo branco e texto preto, que é o formato esperado para a detecção de contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e exibe a imagem original\n",
    "print(f\"Carregando imagem: {IMAGE_PATH}\")\n",
    "original_image = Image.open(IMAGE_PATH)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Imagem Original para OCR')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dimensões: {original_image.size}\")\n",
    "print(f\"Modo: {original_image.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_for_ocr(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Pré-processa a imagem para otimizar a detecção de caracteres.\n",
    "\n",
    "    Esta função executa as seguintes etapas:\n",
    "    1. Converte a imagem para escala de cinza.\n",
    "    2. Aplica um limiar gaussiano adaptativo para criar uma imagem binarizada clara.\n",
    "    3. Garante que a imagem tenha um fundo branco e texto preto, invertendo se necessário,\n",
    "       que é o formato ideal para a detecção de contornos do OpenCV.\n",
    "\n",
    "    Args:\n",
    "        image (Image.Image): O objeto de imagem Pillow de entrada.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A imagem pré-processada como um array NumPy.\n",
    "    \"\"\"\n",
    "    # Converte para escala de cinza\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "\n",
    "    # Converte para numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # Aplica threshold adaptativo para melhor binarização\n",
    "    img_binary = cv2.adaptiveThreshold(\n",
    "        img_array, 255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "\n",
    "    # Verifica se precisa inverter (modelo espera fundo branco, texto preto)\n",
    "    # Se a média é baixa, a imagem está com fundo escuro\n",
    "    if np.mean(img_binary) < 127:\n",
    "        img_binary = 255 - img_binary\n",
    "\n",
    "    return img_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b11006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processa a imagem\n",
    "print(\"Pré-processando imagem...\")\n",
    "processed_image = preprocess_image_for_ocr(original_image)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(processed_image, cmap='gray')\n",
    "plt.title('Imagem Pré-processada (Binarizada)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise e Agrupamento de Contornos\n",
    "\n",
    "Esta é a etapa central do notebook. Primeiro, `analyze_spacing` examina a imagem para encontrar os espaços horizontais entre todos os contornos detectados e sugere um limiar `max_gap`. Em seguida, `segment_characters_with_grouping` usa esse limiar para agrupar contornos próximos em um único \"símbolo\". Isso é crucial para reconhecer corretamente caracteres como \"Ì\" ou \"k\", que são compostos por partes separadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6516cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters_with_grouping(image_array: np.ndarray,\n",
    "                                     max_gap: int = 15,\n",
    "                                     min_component_area: int = 10) -> list:\n",
    "    \"\"\"Segmenta caracteres, agrupando múltiplos contornos próximos em um único símbolo.\n",
    "\n",
    "    Esta função é projetada para lidar com caracteres compostos por partes desconectadas\n",
    "    (como 'i', 'j', ou caracteres com acentos). Ela primeiro encontra todos os contornos,\n",
    "    depois os agrupa horizontalmente se a lacuna (gap) entre eles for menor que `max_gap`.\n",
    "    Finalmente, cria uma única caixa delimitadora e imagem para cada grupo.\n",
    "\n",
    "    Args:\n",
    "        image_array (np.ndarray): A imagem binarizada (fundo branco).\n",
    "        max_gap (int, optional): A distância horizontal máxima entre as caixas delimitadoras\n",
    "                                 para considerá-las parte do mesmo símbolo. O padrão é 15.\n",
    "        min_component_area (int, optional): A área mínima de um contorno para ser considerado\n",
    "                                            um componente válido, para filtrar ruídos. O padrão é 10.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de dicionários, onde cada dicionário representa um símbolo segmentado\n",
    "              e contém 'bbox', 'image', 'components', e 'component_boxes'.\n",
    "    \"\"\"\n",
    "    # 1. Encontra todos os contornos\n",
    "    contours, _ = cv2.findContours(\n",
    "        255 - image_array,\n",
    "        cv2.RETR_EXTERNAL,\n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    # 2. Extrai bounding boxes e filtra ruído\n",
    "    boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = w * h\n",
    "        if area >= min_component_area:\n",
    "            boxes.append({\n",
    "                'x': x, 'y': y, 'w': w, 'h': h,\n",
    "                'x_center': x + w // 2,\n",
    "                'x_end': x + w,\n",
    "                'contour': contour\n",
    "            })\n",
    "\n",
    "    # 3. Ordena boxes da esquerda para direita\n",
    "    boxes = sorted(boxes, key=lambda b: b['x'])\n",
    "\n",
    "    if not boxes:\n",
    "        return []\n",
    "\n",
    "    # 4. Agrupa contornos próximos em símbolos compostos\n",
    "    symbol_groups = []\n",
    "    current_group = [boxes[0]]\n",
    "\n",
    "    for i in range(1, len(boxes)):\n",
    "        prev_box = current_group[-1]\n",
    "        curr_box = boxes[i]\n",
    "\n",
    "        # Calcula gap horizontal entre o fim do anterior e início do atual\n",
    "        gap = curr_box['x'] - prev_box['x_end']\n",
    "\n",
    "        # Se gap é pequeno, agrupa no mesmo símbolo\n",
    "        if gap <= max_gap:\n",
    "            current_group.append(curr_box)\n",
    "        else:\n",
    "            # Gap grande: novo símbolo\n",
    "            symbol_groups.append(current_group)\n",
    "            current_group = [curr_box]\n",
    "\n",
    "    # Adiciona último grupo\n",
    "    symbol_groups.append(current_group)\n",
    "\n",
    "    # 5. Cria bounding box composto para cada grupo\n",
    "    segmented_symbols = []\n",
    "\n",
    "    for group in symbol_groups:\n",
    "        # Calcula bbox que engloba todos os componentes\n",
    "        x_min = min(box['x'] for box in group)\n",
    "        y_min = min(box['y'] for box in group)\n",
    "        x_max = max(box['x'] + box['w'] for box in group)\n",
    "        y_max = max(box['y'] + box['h'] for box in group)\n",
    "\n",
    "        # Adiciona padding\n",
    "        padding = 5\n",
    "        x1 = max(0, x_min - padding)\n",
    "        y1 = max(0, y_min - padding)\n",
    "        x2 = min(image_array.shape[1], x_max + padding)\n",
    "        y2 = min(image_array.shape[0], y_max + padding)\n",
    "\n",
    "        # Extrai região da imagem\n",
    "        symbol_img = image_array[y1:y2, x1:x2]\n",
    "\n",
    "        # Informações do símbolo composto\n",
    "        bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "        component_count = len(group)\n",
    "\n",
    "        segmented_symbols.append({\n",
    "            'bbox': bbox,\n",
    "            'image': symbol_img,\n",
    "            'components': component_count,\n",
    "            'component_boxes': [(b['x'], b['y'], b['w'], b['h']) for b in group]\n",
    "        })\n",
    "\n",
    "    return segmented_symbols\n",
    "\n",
    "\n",
    "def analyze_spacing(image_array: np.ndarray) -> dict:\n",
    "    \"\"\"Analisa o espaçamento horizontal entre contornos para sugerir um `max_gap` ideal.\n",
    "\n",
    "    Esta função calcula a distância horizontal (gap) entre as caixas delimitadoras de todos \n",
    "    os contornos adjacentes. Ela então retorna estatísticas sobre esses gaps, incluindo uma \n",
    "    sugestão para o parâmetro `max_gap` baseada no 25º percentil, que tende a capturar \n",
    "    espaços dentro de caracteres compostos.\n",
    "\n",
    "    Args:\n",
    "        image_array (np.ndarray): A imagem binarizada (fundo branco).\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário contendo a lista de gaps e estatísticas (média, mediana, etc.),\n",
    "              incluindo um `suggested_max_gap`.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(\n",
    "        255 - image_array,\n",
    "        cv2.RETR_EXTERNAL,\n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w * h > 10:\n",
    "            boxes.append({'x': x, 'w': w, 'x_end': x + w})\n",
    "\n",
    "    boxes = sorted(boxes, key=lambda b: b['x'])\n",
    "\n",
    "    # Calcula gaps\n",
    "    gaps = []\n",
    "    for i in range(1, len(boxes)):\n",
    "        gap = boxes[i]['x'] - boxes[i-1]['x_end']\n",
    "        gaps.append(gap)\n",
    "\n",
    "    if not gaps:\n",
    "        return {'gaps': [], 'suggested_max_gap': 15}\n",
    "\n",
    "    gaps = np.array(gaps)\n",
    "\n",
    "    # Estatísticas\n",
    "    stats = {\n",
    "        'gaps': gaps.tolist(),\n",
    "        'mean': float(np.mean(gaps)),\n",
    "        'median': float(np.median(gaps)),\n",
    "        'std': float(np.std(gaps)),\n",
    "        'min': float(np.min(gaps)),\n",
    "        'max': float(np.max(gaps)),\n",
    "        'suggested_max_gap': int(np.percentile(gaps, 25))  # 25º percentil\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86262582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula para análise de espaçamento (execute primeiro para calibrar)\n",
    "print(\"Analisando espaçamento entre contornos...\")\n",
    "spacing_stats = analyze_spacing(processed_image)\n",
    "\n",
    "print(f\"\\nEstatísticas de gaps:\")\n",
    "print(f\"  Média: {spacing_stats['mean']:.1f} pixels\")\n",
    "print(f\"  Mediana: {spacing_stats['median']:.1f} pixels\")\n",
    "print(f\"  Desvio padrão: {spacing_stats['std']:.1f} pixels\")\n",
    "print(f\"  Min/Max: {spacing_stats['min']:.0f} / {spacing_stats['max']:.0f} pixels\")\n",
    "print(f\"  ✓ max_gap sugerido: {spacing_stats['suggested_max_gap']} pixels\")\n",
    "\n",
    "# Visualiza distribuição de gaps\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(spacing_stats['gaps'], bins=20, edgecolor='black')\n",
    "plt.axvline(spacing_stats['suggested_max_gap'], color='r', linestyle='--',\n",
    "            label=f\"Sugestão: {spacing_stats['suggested_max_gap']}px\")\n",
    "plt.xlabel('Gap (pixels)')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Distribuição de Espaçamento entre Contornos')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9454fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula para segmentação com agrupamento\n",
    "print(\"\\nSegmentando símbolos compostos...\")\n",
    "\n",
    "# Use o max_gap sugerido ou ajuste manualmente\n",
    "MAX_GAP = spacing_stats['suggested_max_gap']  # ou um valor fixo como 15\n",
    "\n",
    "segmented_symbols = segment_characters_with_grouping(\n",
    "    processed_image,\n",
    "    max_gap=MAX_GAP,\n",
    "    min_component_area=10\n",
    ")\n",
    "\n",
    "print(f\"✓ {len(segmented_symbols)} símbolos detectados\")\n",
    "\n",
    "# Visualiza símbolos com informação de componentes\n",
    "if len(segmented_symbols) > 0:\n",
    "    n_symbols = len(segmented_symbols)\n",
    "    cols = min(10, n_symbols)\n",
    "    rows = (n_symbols + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(15, 2 * rows))\n",
    "    for i, symbol in enumerate(segmented_symbols):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(symbol['image'], cmap='gray')\n",
    "        plt.title(f\"#{i+1}\\n{symbol['components']} comp.\", fontsize=9)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f'Símbolos Segmentados (max_gap={MAX_GAP}px)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Estatísticas\n",
    "    comp_counts = [s['components'] for s in segmented_symbols]\n",
    "    print(f\"\\nComponentes por símbolo:\")\n",
    "    print(f\"  Média: {np.mean(comp_counts):.1f}\")\n",
    "    print(f\"  Min/Max: {min(comp_counts)} / {max(comp_counts)}\")\n",
    "else:\n",
    "    print(\"⚠ Nenhum símbolo detectado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação de Símbolos para Previsão\n",
    "\n",
    "Cada imagem de símbolo agrupado é processada pela função `prepare_char_for_prediction`. Ela centraliza o símbolo em uma tela quadrada, redimensiona-o para o tamanho de entrada do modelo e adiciona as dimensões de lote e canal necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec798869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_char_for_prediction(char_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Prepara a imagem de um caractere segmentado para a predição do modelo.\n",
    "\n",
    "    Esta função pega a imagem de um único caractere, a centraliza em uma tela quadrada \n",
    "    (canvas) branca, redimensiona para as dimensões esperadas pelo modelo (64x64) e \n",
    "    remodela o array para o formato de entrada do modelo (1, 64, 64, 1).\n",
    "\n",
    "    Args:\n",
    "        char_image (np.ndarray): A imagem do caractere segmentado (fundo branco).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A imagem preparada, pronta para ser usada como entrada para o \n",
    "                    modelo de previsão.\n",
    "    \"\"\"\n",
    "    # Cria canvas quadrado branco\n",
    "    size = max(char_image.shape)\n",
    "    canvas = np.ones((size, size), dtype=np.uint8) * 255\n",
    "\n",
    "    # Centraliza o caractere\n",
    "    y_offset = (size - char_image.shape[0]) // 2\n",
    "    x_offset = (size - char_image.shape[1]) // 2\n",
    "    canvas[y_offset:y_offset+char_image.shape[0],\n",
    "           x_offset:x_offset+char_image.shape[1]] = char_image\n",
    "\n",
    "    # Redimensiona para 64x64\n",
    "    resized = cv2.resize(canvas, (IMG_WIDTH, IMG_HEIGHT),\n",
    "                        interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # Adiciona dimensões: (1, 64, 64, 1)\n",
    "    # IMPORTANTE: NÃO normalizar aqui - o modelo tem Rescaling(1./255) interno\n",
    "    prepared = resized.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do OCR e Exibição dos Resultados\n",
    "\n",
    "O notebook itera sobre cada símbolo segmentado, o prepara para o modelo e executa a previsão. O caractere com a maior confiança é selecionado. O resultado final, incluindo o texto reconhecido, a confiança de cada previsão e o número de componentes, é impresso na tela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula para OCR com os símbolos agrupados\n",
    "print(\"\\nRealizando OCR dos símbolos compostos...\")\n",
    "results = []\n",
    "\n",
    "for i, symbol_data in enumerate(segmented_symbols):\n",
    "    bbox = symbol_data['bbox']\n",
    "    char_img = symbol_data['image']\n",
    "\n",
    "    # Prepara para predição\n",
    "    input_array = prepare_char_for_prediction(char_img)\n",
    "\n",
    "    # Predição\n",
    "    predictions = model.predict(input_array, verbose=0)\n",
    "    predicted_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_idx]\n",
    "\n",
    "    # Converte para caractere\n",
    "    unicode_class = class_names[predicted_idx]\n",
    "    character = unicode_to_char(unicode_class)\n",
    "\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'bbox': bbox,\n",
    "        'unicode': unicode_class,\n",
    "        'character': character,\n",
    "        'confidence': confidence,\n",
    "        'components': symbol_data['components'],\n",
    "        'component_boxes': symbol_data['component_boxes'],\n",
    "        'image': char_img\n",
    "    })\n",
    "\n",
    "    comp_info = f\" ({symbol_data['components']} componentes)\" if symbol_data['components'] > 1 else \"\"\n",
    "    print(f\"Símbolo {i+1}: '{character}' ({unicode_class}){comp_info} - Confiança: {confidence:.4f}\")\n",
    "\n",
    "recognized_text = ''.join([r['character'] for r in results])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEXTO RECONHECIDO: {recognized_text}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
